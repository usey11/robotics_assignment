{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from tensorrt_model import TRTModel\n",
    "from ssd_tensorrt import load_plugins, parse_boxes,TRT_INPUT_NAME, TRT_OUTPUT_NAME\n",
    "import ctypes\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import ctypes\n",
    "\n",
    "\n",
    "mean = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "stdev = 255.0 * np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "\n",
    "def bgr8_to_ssd_input(camera_value):\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1)).astype(np.float32)\n",
    "    x -= mean[:, None, None]\n",
    "    x /= stdev[:, None, None]\n",
    "    return x[None, ...]\n",
    "\n",
    "class ObjectDetector(object):\n",
    "    \n",
    "    def __init__(self, engine_path, preprocess_fn=bgr8_to_ssd_input):\n",
    "        logger = trt.Logger()\n",
    "        trt.init_libnvinfer_plugins(logger, '')\n",
    "        load_plugins()\n",
    "        self.trt_model = TRTModel(engine_path, input_names=[TRT_INPUT_NAME],output_names=[TRT_OUTPUT_NAME, TRT_OUTPUT_NAME + '_1'])\n",
    "        self.preprocess_fn = preprocess_fn\n",
    "        \n",
    "    def execute(self, *inputs):\n",
    "        trt_outputs = self.trt_model(self.preprocess_fn(*inputs))\n",
    "        return parse_boxes(trt_outputs)\n",
    "    \n",
    "    def __call__(self, *inputs):\n",
    "        return self.execute(*inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use traitlets and widgets to display the image in Jupyter Notebook\n",
    "import traitlets\n",
    "from traitlets.config.configurable import SingletonConfigurable\n",
    "\n",
    "#use opencv to covert the depth image to RGB image for displaying purpose\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#using realsense to capture the color and depth image\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "#multi-threading is used to capture the image in real time performance\n",
    "import threading\n",
    "\n",
    "class Camera(SingletonConfigurable):\n",
    "    \n",
    "    #this changing of these two values will be captured by traitlets\n",
    "    color_value = traitlets.Any()\n",
    "    depth_value = traitlets.Any()\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Camera, self).__init__()\n",
    "        \n",
    "        #configure the color and depth sensor\n",
    "        self.pipeline = rs.pipeline()\n",
    "        self.configuration = rs.config()  \n",
    "        \n",
    "        #set resolubg_removedtion for the color camera\n",
    "        self.color_width = 640\n",
    "        self.color_height = 480\n",
    "        self.color_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.color, self.color_width, self.color_height, rs.format.bgr8, self.color_fps)\n",
    "\n",
    "        #set resolution for the depth camera\n",
    "        self.depth_width = 640\n",
    "        self.depth_height = 480\n",
    "        self.depth_fps = 30\n",
    "        self.configuration.enable_stream(rs.stream.depth, self.depth_width, self.depth_height, rs.format.z16, self.depth_fps)\n",
    "\n",
    "        self.stop_flag = False\n",
    "        \n",
    "        #Create an align object\n",
    "        self.align = rs.align(rs.stream.color)\n",
    "        \n",
    "        #start capture \n",
    "        profile = self.pipeline.start(self.configuration)\n",
    "        \n",
    "        # clipping distance \n",
    "        depth_sensor = profile.get_device().first_depth_sensor()\n",
    "        depth_scale = depth_sensor.get_depth_scale()\n",
    "                        \n",
    "        #the first depth and color image\n",
    "        self.pipeline_started = True\n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "        \n",
    "        # align the frames\n",
    "        aligned_frames = self.align.process(frames)\n",
    "\n",
    "        color_frame = aligned_frames.get_color_frame()   \n",
    "        image = np.asanyarray(color_frame.get_data())\n",
    "        self.color_value = image\n",
    "\n",
    "        depth_frame = aligned_frames.get_depth_frame()           \n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "        self.depth_value = depth_image    \n",
    "\n",
    "    def _capture_frames(self):\n",
    "        while(self.stop_flag==False):\n",
    "            frames = self.pipeline.wait_for_frames()\n",
    "            \n",
    "            # align the frames\n",
    "            aligned_frames = self.align.process(frames)\n",
    "            \n",
    "            color_frame = aligned_frames.get_color_frame()\n",
    "            image = np.asanyarray(color_frame.get_data())\n",
    "            \n",
    "\n",
    "            depth_frame = aligned_frames.get_depth_frame()              \n",
    "            depth_image = np.asanyarray(depth_frame.get_data())            \n",
    "            \n",
    "            self.color_value = image\n",
    "            \n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            self.depth_value = depth_colormap\n",
    "            self.depth_image = depth_image\n",
    "            \n",
    "            \n",
    "    def start(self):\n",
    "        if not hasattr(self, 'thread') or not self.thread.isAlive():\n",
    "            self.thread = threading.Thread(target=self._capture_frames)\n",
    "            self.thread.start()\n",
    "\n",
    "    def stop(self):\n",
    "        self.stop_flag = True\n",
    "        if hasattr(self, 'thread'):\n",
    "            self.thread.join()        \n",
    "        if self.pipeline_started:\n",
    "            self.pipeline.stop()\n",
    "\n",
    "def bgr8_to_jpeg(value,quality=75):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "#create a camera object\n",
    "camera = Camera.instance()\n",
    "camera.start() # start capturing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d147a1e39d0f4681b6632d550b3b4e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg'), Image(value=b'', format='jpeg'))), IntText(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "from mochris import RangeSensors\n",
    "\n",
    "range_sensors = RangeSensors(front=True, front_range=2, front_roi=3, fiat=0)\n",
    "\n",
    "width = 640\n",
    "height = 480\n",
    "\n",
    "display_depth = widgets.Image(format='jpeg') #, height=480)\n",
    "image_widget = widgets.Image(format='jpeg') #, height=300)\n",
    "\n",
    "label_widget = widgets.IntText(value=1, description='tracked label')\n",
    "\n",
    "distance_slider = widgets.IntSlider(description='Obstacle', min=40, max=4000, orientation='horizontal')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, display_depth]),\n",
    "    label_widget,\n",
    "    distance_slider\n",
    "]))\n",
    "\n",
    "import time\n",
    "from RobotClass import Robot\n",
    "\n",
    "#initialize the Robot class\n",
    "robot = Robot()\n",
    "bbox = None\n",
    "\n",
    "MIN_LEFT = 270\n",
    "MIN_RIGHT = 370\n",
    "TURN_SPEED = 0.4\n",
    "\n",
    "FORWARD_SPEED = 0.4\n",
    "FOLOW_DEPTH = 900\n",
    "\n",
    "def processing(change):\n",
    "    image = change['new']\n",
    "    imgsized= cv2.resize(image,(300,300))\n",
    "    # compute all detected objects\n",
    "    detections = model(imgsized)\n",
    "    \n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    \n",
    "    for det in matching_detections:\n",
    "        bbox = det['bbox']\n",
    "            \n",
    "        start_point = (int(width * bbox[0]), int(height * bbox[1]))\n",
    "        end_point = (int(width * bbox[2]), int(height * bbox[3]))\n",
    "        \n",
    "        center = (int((start_point[0] + end_point[0])/2), int((start_point[1] + end_point[1])/2))\n",
    "        \n",
    "        cv2.rectangle(image, start_point, end_point, (255, 0, 0), 2)\n",
    "       \n",
    "    if len(matching_detections)==0:\n",
    "        robot.stop()\n",
    "    elif center[0] >= 0 and center[0] < width and center[1] >= 0 and center[1] < height:\n",
    "        global distance_slider\n",
    "        depth_box = camera.depth_image[start_point[1]:end_point[1], start_point[0]:end_point[0]]\n",
    "        \n",
    "        depth_box[depth_box<100] = 0\n",
    "        depth_box[depth_box>5000] = 0\n",
    "        \n",
    "        depth_box = depth_box[depth_box!=0]\n",
    "        \n",
    "        depth = 0\n",
    "        if len(depth_box) != 0:\n",
    "            depth = depth_box.min()\n",
    "            distance_slider.value = depth\n",
    "        \n",
    "        cv2.circle(image, center, 5, (0,0,255), 2)\n",
    "        \n",
    "        image[center[1],center[0]] = (0,0,255)\n",
    "        \n",
    "        if center[0] < MIN_LEFT:\n",
    "            robot.left(TURN_SPEED)\n",
    "        elif center[0] > MIN_RIGHT:\n",
    "            robot.right(TURN_SPEED)\n",
    "        elif depth != 0 and depth > FOLOW_DEPTH:\n",
    "            robot.forward(FORWARD_SPEED)\n",
    "        else:\n",
    "            robot.stop()\n",
    "        \n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "#the camera.observe function will monitor the color_value variable. If this value changes, the excecute function will be excuted.\n",
    "processing({'new': camera.color_value})\n",
    "camera.observe(processing, names='color_value')\n",
    "\n",
    "\n",
    "def bgr8_to_jpeg(value,quality=75):\n",
    "    return bytes(cv2.imencode('.jpg',value)[1])\n",
    "\n",
    "\n",
    "#link the camera.depth_value to the display_depth\n",
    "camera_link_depth = traitlets.dlink((camera, 'depth_value'), (display_depth, 'value'), transform=bgr8_to_jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "            #we only consider the central area of the vision sensor\n",
    "            depth_image[:190,:]=0\n",
    "            depth_image[290:,:]=0\n",
    "            depth_image[:,:160]=0\n",
    "            depth_image[:,480:]=0\n",
    "            \n",
    "            #For object avoidance, we don't consider the distance that are lower than 100mm or bigger than 1000mm\n",
    "            depth_image[depth_image<100]=0\n",
    "            depth_image[depth_image>1000]=0\n",
    "            \n",
    "            #If all of the values in the depth image is 0, the depth[depth!=0] command will fail\n",
    "            #we set a specific value here to prevent this failure\n",
    "            depth_image[0,0]=2000\n",
    "\n",
    "#             ss=np.zeros((480,640),dtype=np.uint8)    \n",
    "#             ss[depth_image!=0]=255\n",
    "#             cv2.imshow('ss',ss)\n",
    "            \n",
    "            depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "            if (depth_image[depth_image!=0].min()<500):\n",
    "                cv2.putText(depth_colormap, 'warning!!!', (320,240), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "                self.warning_flag=1\n",
    "            else:\n",
    "                self.warning_flag=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
